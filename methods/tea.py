"""
Energy-Based Test-Time Adaptation
Adapts a model by matching the energy distribution of test data with model-generated samples.

This implementation adapts the method from the paper:
"Generative-Contrastive-Representation-Learning-for-Out-of-Distribution-Detection-and-Adaptation"
and integrates it into the TTA framework.
"""

from copy import deepcopy
import torch
import torch.jit
import torch.nn as nn
import torch.nn.functional as F

from methods.base import TTAMethod
from utils.registry import ADAPTATION_REGISTRY

# --- Helper Functions and Classes from energy.py ---

def init_random(bs, im_sz=32, n_ch=3):
    """Initializes a batch of random noise."""
    return torch.FloatTensor(bs, n_ch, im_sz, im_sz).uniform_(-1, 1)

class EnergyModel(nn.Module):
    """A wrapper for the model to compute energy scores."""
    def __init__(self, model):
        super(EnergyModel, self).__init__()
        self.f = model

    def classify(self, x):
        """Computes logits for classification."""
        return self.f(x)
    
    def forward(self, x, y=None):
        """
        Computes the energy of the input.
        If y is None, returns the log-sum-exp of logits (unconditional energy).
        If y is provided, returns the logit corresponding to the given class (conditional energy).
        """
        logits = self.classify(x)
        if y is None:
            return logits.logsumexp(1), logits
        else:
            return torch.gather(logits, 1, y[:, None]), logits

def sample_p_0(reinit_freq, replay_buffer, bs, im_sz, n_ch, device):
    """
    Samples initial points for Langevin dynamics.
    A mix of samples from the replay buffer and random noise.
    """
    if len(replay_buffer) == 0:
        return init_random(bs, im_sz=im_sz, n_ch=n_ch).to(device), []
    
    buffer_size = len(replay_buffer)
    inds = torch.randint(0, buffer_size, (bs,))
    buffer_samples = replay_buffer[inds]
    random_samples = init_random(bs, im_sz=im_sz, n_ch=n_ch)
    
    choose_random = (torch.rand(bs) < reinit_freq).float()[:, None, None, None]
    samples = choose_random * random_samples + (1 - choose_random) * buffer_samples
    return samples.to(device), inds

def sample_q(f, replay_buffer, n_steps, sgld_lr, sgld_std, reinit_freq, batch_size, im_sz, n_ch, device, y=None):
    """
    Generates samples from the energy model using Stochastic Gradient Langevin Dynamics (SGLD).
    """
    f.eval()  # Set to eval for SGLD sampling
    
    # Get initial samples
    init_sample, buffer_inds = sample_p_0(reinit_freq, replay_buffer, batch_size, im_sz, n_ch, device)
    x_k = torch.autograd.Variable(init_sample, requires_grad=True)
    
    # SGLD loop
    for _ in range(n_steps):
        # The gradient of the energy function is computed w.r.t. the input samples
        f_prime = torch.autograd.grad(f(x_k, y=y)[0].sum(), [x_k], retain_graph=True)[0]
        x_k.data += sgld_lr * f_prime + sgld_std * torch.randn_like(x_k)
        
    f.train()  # Return to train mode
    final_samples = x_k.detach()
    
    # Update replay buffer
    if len(replay_buffer) > 0 and len(buffer_inds) > 0:
        replay_buffer[buffer_inds] = final_samples.cpu()
        
    return final_samples


# --- Main Energy Class for TTA Framework ---

@ADAPTATION_REGISTRY.register()
class TEA(TTAMethod):
    """
    Energy-based adaptation method.
    Adapts the model by minimizing the contrastive divergence between the energy of 
    real-world test samples and synthetic samples generated by the model.
    """
    def __init__(self, cfg, model, num_classes):
        super().__init__(cfg, model, num_classes)

        # Wrap the model to get an energy model
        self.energy_model = EnergyModel(self.model)

        # Get hyperparameters for the energy-based method from the config
        self.sgld_steps = 20
        self.sgld_lr = 1.0
        self.sgld_std = 0.01
        self.reinit_freq = 0.05
        self.if_cond = False

        # Image properties (assuming they are in cfg.TEST or cfg.DATA)
        self.im_sz = 32 # Assuming 32x32 images, can be adjusted based on dataset
        self.n_ch = 3 # Assuming 3 channels

        # Initialize the replay buffer for storing generated samples
        buffer_size = 10000
        self.replay_buffer = init_random(buffer_size, im_sz=self.im_sz, n_ch=self.n_ch)

    def loss_calculation(self, x):
        """
        Calculates the energy-based contrastive loss.
        """
        imgs_test = x[0]
        batch_size = imgs_test.shape[0]
        device = imgs_test.device

        # Determine if conditional generation is needed
        y = None
        if self.if_cond:
            y = torch.randint(0, self.num_classes, (batch_size,)).to(device)

        # 1. Generate synthetic samples (x_fake) using SGLD
        x_fake = sample_q(self.energy_model, self.replay_buffer,
                          n_steps=self.sgld_steps, sgld_lr=self.sgld_lr,
                          sgld_std=self.sgld_std, reinit_freq=self.reinit_freq,
                          batch_size=batch_size, im_sz=self.im_sz, n_ch=self.n_ch,
                          device=device, y=y)

        # 2. Compute energy for real and fake samples
        out_real, logits_real = self.energy_model(imgs_test)
        energy_real = out_real.mean()
        energy_fake = self.energy_model(x_fake)[0].mean()

        # 3. Calculate the contrastive loss
        # We want to decrease the energy of real data and increase the energy of fake data
        loss = energy_real - energy_fake
        
        # Return the logits for the original input and the calculated loss
        return logits_real, loss

    @torch.enable_grad()
    def forward_and_adapt(self, x):
        """
        Forward pass and adaptation step.
        The model is updated by backpropagating the energy-based loss.
        """
        if self.mixed_precision and self.device == "cuda":
            with torch.cuda.amp.autocast():
                outputs, loss = self.loss_calculation(x)
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
        else:
            outputs, loss = self.loss_calculation(x)
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
        return outputs

    # Corrected Code
    def collect_params(self):
        """
        Collects all trainable parameters of the model.
        Unlike Tent, this method adapts the entire model.
        """
        # Convert the generator to a list before returning
        params = list(self.model.parameters())
        names = [name for name, _ in self.model.named_parameters()]
        return params, names

    def configure_model(self):
        # train mode, because tent optimizes the model to minimize entropy
        # self.model.train()
        self.model.eval()  # eval mode to avoid stochastic depth in swin. test-time normalization is still applied
        # disable grad, to (re-)enable only what tent updates
        self.model.requires_grad_(False)
        # configure norm for tent updates: enable grad + force batch statisics
        for m in self.model.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.requires_grad_(True)
                # force use of batch stats in train and eval modes
                m.track_running_stats = False
                m.running_mean = None
                m.running_var = None
            elif isinstance(m, nn.BatchNorm1d):
                m.train()   # always forcing train mode in bn1d will cause problems for single sample tta
                m.requires_grad_(True)
            elif isinstance(m, (nn.LayerNorm, nn.GroupNorm)):
                m.requires_grad_(True)
    # def configure_model(self):
    #     """
    #     Configures the model for adaptation.
    #     Sets the model to train mode and ensures all parameters require gradients.
    #     """
    #     self.model.train()
    #     self.model.requires_grad_(True)